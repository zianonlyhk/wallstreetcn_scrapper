#!/usr/bin/env python3
"""
Performance comparison script for the optimized wallstreetcn scraper
"""

import time
import sys
from src.get_news_list import get_news_entries_as_json
from src.get_news_content import get_news_data, get_news_data_sequential

def test_news_list_performance():
    """Test the performance of news list scraping"""
    print("=" * 60)
    print("TESTING NEWS LIST SCRAPING PERFORMANCE")
    print("=" * 60)
    
    # Test with different time filters
    time_filters = [2, 6, 12, 24]
    
    for time_filter in time_filters:
        print(f"\nTesting with time_filter={time_filter} hours:")
        start_time = time.time()
        
        try:
            result = get_news_entries_as_json(time_filter=time_filter)
            end_time = time.time()
            duration = end_time - start_time
            
            # Count number of articles
            import json
            news_data = json.loads(result)
            article_count = len(news_data) if isinstance(news_data, list) else 0
            
            print(f"  âœ“ Found {article_count} articles in {duration:.2f} seconds")
            print(f"  âœ“ Rate: {article_count/duration:.1f} articles/second")
            
        except Exception as e:
            print(f"  âœ— Error: {str(e)}")

def test_content_scraping_performance():
    """Test the performance of content scraping"""
    print("\n" + "=" * 60)
    print("TESTING CONTENT SCRAPING PERFORMANCE")
    print("=" * 60)
    
    # Sample URLs for testing
    test_urls = [
        "https://wallstreetcn.com/articles/3747562",
        "https://wallstreetcn.com/articles/3747573", 
        "https://wallstreetcn.com/articles/3747570"
    ]
    
    print(f"\nTesting with {len(test_urls)} URLs:")
    for i, url in enumerate(test_urls, 1):
        print(f"  {i}. {url}")
    
    # Test concurrent version
    print("\n--- Concurrent Processing ---")
    start_time = time.time()
    try:
        concurrent_results = get_news_data(test_urls, max_workers=3)
        concurrent_time = time.time() - start_time
        concurrent_success = len([r for r in concurrent_results if not r.startswith("Error")])
        
        print(f"  âœ“ Concurrent: {concurrent_success}/{len(test_urls)} articles in {concurrent_time:.2f} seconds")
        print(f"  âœ“ Rate: {concurrent_success/concurrent_time:.1f} articles/second")
    except Exception as e:
        print(f"  âœ— Concurrent Error: {str(e)}")
        concurrent_time = float('inf')
    
    # Test sequential version for comparison
    print("\n--- Sequential Processing ---")
    start_time = time.time()
    try:
        sequential_results = get_news_data_sequential(test_urls)
        sequential_time = time.time() - start_time
        sequential_success = len([r for r in sequential_results if not r.startswith("Error")])
        
        print(f"  âœ“ Sequential: {sequential_success}/{len(test_urls)} articles in {sequential_time:.2f} seconds")
        print(f"  âœ“ Rate: {sequential_success/sequential_time:.1f} articles/second")
        
        # Calculate improvement
        if concurrent_time != float('inf') and sequential_time > 0:
            improvement = (sequential_time - concurrent_time) / sequential_time * 100
            speedup = sequential_time / concurrent_time
            print(f"\n  ðŸš€ Performance Improvement:")
            print(f"     â€¢ {improvement:.1f}% faster")
            print(f"     â€¢ {speedup:.1f}x speedup")
    except Exception as e:
        print(f"  âœ— Sequential Error: {str(e)}")

def main():
    """Run all performance tests"""
    print("WALLSTREETCN SCRAPER PERFORMANCE TEST")
    print("=" * 60)
    print("Testing optimized scraper performance...")
    
    # Test news list scraping
    test_news_list_performance()
    
    # Test content scraping
    test_content_scraping_performance()
    
    print("\n" + "=" * 60)
    print("OPTIMIZATION SUMMARY")
    print("=" * 60)
    print("Key optimizations implemented:")
    print("â€¢ Reduced WebDriver creation overhead")
    print("â€¢ Concurrent content scraping (3x faster typical)")
    print("â€¢ Optimized Chrome options (disabled images, CSS, JS)")
    print("â€¢ Reduced wait times and timeouts")
    print("â€¢ More efficient element selection")
    print("â€¢ Pre-compiled regex patterns")
    print("â€¢ Improved scrolling logic")
    print("â€¢ Thread-local WebDriver instances")

if __name__ == "__main__":
    main()
